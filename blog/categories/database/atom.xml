<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Database | Ivan Vari]]></title>
  <link href="http://variia.github.io/blog/categories/database/atom.xml" rel="self"/>
  <link href="http://variia.github.io/"/>
  <updated>2016-10-18T07:43:37+02:00</updated>
  <id>http://variia.github.io/</id>
  <author>
    <name><![CDATA[Ivan Vari]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Couchbase Quota Explained]]></title>
    <link href="http://variia.github.io/couchbase-quota-explained/"/>
    <updated>2013-10-16T00:28:13+02:00</updated>
    <id>http://variia.github.io/couchbase-quota-explained</id>
    <content type="html"><![CDATA[<p>For modern, high performance web applications we need low latency and Couchbase excels in that. To maintain the lowest possible latency even during node failure,
we need to achieve 100% resident ratio for our high performance buckets. This means that Couchbase serves all your data from RAM, even the least frequently accessed ones,
disk is used for persistence only. It turns out that in this condition your usable RAM is lot less, 2 thirds of your allocated quota.</p>

<!--more-->


<p>I always found it hard to come up with a number for bucket quota. It depends on the key size which can vary, the number of keys expected to be stored, your expiry
on keys, etc. So what we do is essentially oversize the buckets initially (as long as we can afford it with RAM of course), then observe and perhaps adjust it later on
but this gets harder when you start utilising your cluster RAM.</p>

<p><img src="http://variia.github.io/images/2013-10/CFDD97C6-310B-11E3-AD2F-001D095D855C.jpg" /></p>

<p>I had a solid understanding of the &ldquo;mem_low_wat&rdquo; and &ldquo;mem_high_wat&rdquo; watermarks, I just didn&rsquo;t know their impact and how their values are calculated.
The <a href="http://www.couchbase.com/docs/couchbase-manual-2.0/couchbase-introduction-architecture-ejection-eviction.html" target="_blank">working set management guide</a>
explains this, I am not going to repeat what is discussed in that, just reflect what happens in real life.</p>

<p>In my case the default ratio (per bucket):</p>

<p><strong>mem_low_wat = ~60%</strong> (of your dynamic RAM quota)</p>

<p><strong>mem_high_wat = ~75%</strong> (of your dynamic RAM quota)</p>

<blockquote><p>In nutshell: when a certain amount of RAM is consumed by items, the server will eject items starting with replica data. This threshold is known as the low water mark.
If higher threshold is breached, Couchbase Server will not only eject replica data, it will also eject less-frequently used items.</p>

<p>This ejection happens at node level.</p></blockquote>

<p>I have always monitored both, the resident ratios and of course the quotas but missed the &ldquo;low&rdquo; and especially the &ldquo;high&rdquo; watermarks. This turned out to be problematic
in the last couple of weeks, our cluster lost it&rsquo;s harmony and we started fetching from disk which adds an extra 100-200us latency to our average GETs occasionally.
One of my buckets is somewhat large, currently the quota is set to 100GB (400GB across 4 nodes) and the bucket holds 500+ Million keys. If you consider the ratio above,
the default behaviour sets the &ldquo;mem_high_wat&rdquo; to 100GB, that is a lot of RAM to be wasted!</p>

<p><strong>Explanation:</strong></p>

<p>In layman&rsquo;s term:  it&rsquo;s a buffer, it&rsquo;s a protection against unexpected large write spike, node failure, manual failover, rebalance, etc. If the bucket&rsquo;s memory usage
reaches 90% of its quota allocation, the application would start throwing &ldquo;temporary out of memory errors&rdquo; which would of course need to be handled some ways.
If an application is not coded to handle this type of exception, writes/reads may fail.</p>

<p>While these watermarks can be changed on per node / per bucket basis, it&rsquo;s strongly recommended not to do so as it can potentially put your data at risk.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Couchbase 1.8 Persistence]]></title>
    <link href="http://variia.github.io/couchbase-1-8-persistence/"/>
    <updated>2013-05-21T13:34:53+02:00</updated>
    <id>http://variia.github.io/couchbase-1-8-persistence</id>
    <content type="html"><![CDATA[<p><a href="http://www.couchbase.com" target="_blank">Couchbase</a> 1.8 supports two types of buckets but the <em>memcached</em> bucket is limited, does not
support persistence, failover so this article is about the <em>couchbase</em> bucket type and its maintenance.</p>

<p>We tend to forget the fact, that this bucket is persisted so every single key is saved to disk. This means you have a copy in memory (assume your
resident ratio is 100%) and on disk. Depending on your cluster setup, you will likely to have at least another copy in another node&rsquo;s memory
and its disk. (4 copies altogether)</p>

<!--more-->


<p>With the added metadata overhead, it&rsquo;s fair to say that you actually need more disk space on each node than memory, to be able to fully utilise your
node&rsquo;s memory and you have to consider this when you size your hardware. Couchbase 2.x requires even more disk-space (2 x your RAM) per node due to
the JSON indexes and changed persistence layer.</p>

<h2>Update</h2>

<p>Note: the behavior/technique explained here only true up to a certain size, aka vacuum is only practical for smaller databases. For large databases
(10G+ per file), it&rsquo;s much more efficient to fail over the node then add it back to the cluster followed by rebalance.</p>

<h2>The Keys</h2>

<p>Depending on what you use a <em>key-value</em> store for, it&rsquo;s fairly common, that the data you store changes frequently and how long it will remain in your
bucket depends on the expiry you set on creation.</p>

<p><em>You have to nail the expiry to prevent bloating the store or deleting important dataset too early</em></p>

<p>We started loading one of our high performance buckets and set the expiry initially to 30 days. Unfortunately we didn&rsquo;t realize at that point, that we
get more data from web visitors, than we delete (by expiry) so our bucket got bloated with approximately 500 - 600 Million keys. (39G in memory, 45G on
disk per node) This pushed our resident ratio down to 24% approximately, which was not really an issue until we started actually fetching data for our
testing.</p>

<h2>The Cleanup</h2>

<p>We could have raised the bucket quota since we had free RAM, but we knew the bucket was bloated with stale data, so the only option we had was to clean
it up. We set the expiry immediately in our code to 7 days, but it only affected newly created keys so we had to wait 4 weeks before we actually saw
our store reducing in size.</p>

<p><img src="http://variia.github.io/images/2013-05/971B1CF6-BE65-11E2-9FAE-001D095D855C.png" /></p>

<p>Our bucket <strong>started reducing</strong> in size around January <strong>but only in memory</strong>, the green area representing the disk usage remained the same.</p>

<p>By early February we reached our target size, but our disk was still bloated and it had to be claimed back, although it was not as simple as you may think.</p>

<h2>SQL Fragmentation</h2>

<p>Couchbase 1.8 uses SQLite3 to persist the data, it comes with the product installer package. The <a href="http://support.couchbase.com/entries/21724787-Understanding-SQLite-fragmentation-and-Vacuuming-of-databases" target="_blank">knowledgebase</a>
(login required) from the support portal explains this very well although in real life scenario, I believe there is a little more to it.</p>

<h2>The not so easy way</h2>

<p>Make sure you have <em>auto-failover</em> set, remove the node to be cleaned followed by <em>rebalance</em>. When finished add the node back to the cluster and complete <em>rebalance</em> again.
When you initiate the <em>rebalance</em> operation the entire Couchbase data area on your disk is deleted, keys and metadata will be synced across all live nodes.</p>

<p><em>In production, even off peak, this is a very heavy operation</em></p>

<p><strong>Every node</strong> will be hitting the disk hard, CPU usage will increase, response times and latency will be poor. One node took approximately 2 x 20 minutes,
on our small 4 node cluster it&rsquo;s 160 minutes and it&rsquo;s best case scenario. When fully utilized, this could go up as high as 12 hours just to clean our 4 node
cluster and it&rsquo;s worth to mention that we have the fastest SSD available.
The final important aspect of this method is that rebalance fails sometimes, there is a lot going on and it&rsquo;s somewhat <em>normal</em> according to 
<a href="http://blog.couchbase.com/top-10-things-ops-sys-admin-must-know-about-couchbase" target="_blank">another article</a>. When it does you have to start
the <em>rebalance</em> all over again, and it could go on for some time until you get all your nodes cleaned. This is something we could not afford, it&rsquo;s just not
feasible for 24/7 operations.</p>

<h2>Manual fragmentation</h2>

<p>The idea is that you essentially make the data unavailable on a single node until the <em>VACUUM</em> runs then you just suck the data back into memory from the
de-fragmented store. Yes, your active-replica will not be activated and in our case (4 node cluster) 25% of our data will not be accessible at all but for us
it is better than having uselessly slow 100% while <em>rebalance</em> is running.</p>

<p>This requires <em>auto failover</em> to be turned off, you can do it by a simple API call:</p>

<pre><code class="bash in disable failover:">$ curl "http://localhost:8091/settings/autoFailover" -i -u Administrator:"yourpassword" -d 'enabled=false'
</code></pre>

<p>followed by shutting the <em>coushbase-server</em> process down. When your server process is stopped, you can then <em>VACUUM</em> the database files individually as explained
in the <a href="http://support.couchbase.com/entries/21724787-Understanding-SQLite-fragmentation-and-Vacuuming-of-databases" target="_blank">knowledgebase</a> article.</p>

<h2>The surprise</h2>

<p>It may fail with the following error:</p>

<p><code>Error: disk I/O error</code></p>

<p>After some reading and digging, we found that SQLite3 makes a copy of your database on the fly to <code>/var/tmp</code> then runs the VACUUM over the temporary copy, on success
it copies the de-fragmented database back to its place. Our database files were ~35-40GB each and as much as we pay attention to partitioning when we build servers,
we did not count that into our sizing. Not to mention that <code>/var/tmp</code> was not on SSD storage so the VACUUM was slow too, hence we ended up with the following fix for
our disk layout:</p>

<pre><code class="bash mount:">/dev/sdb1 on /opt type ext4 (rw,noatime,data=writeback,commit=120)
/opt/temp on /var/tmp type none (rw,bind)
</code></pre>

<p>Couchbase lives under <code>/opt</code> default so we mounted our fast SSD disk (sdb) there during installation, thus we just added some performance tuning options to our EXT4
file system.</p>

<p>Then we created a temporary folder at <code>/opt/temp</code> and mounted <code>/var/tmp</code> to it. This way we got enough space to complete the database <em>VACUUM</em> and we moved this high
IO operation to our fastest drive available.</p>

<p>At completion, start the server process and <a href="http://www.couchbase.com/docs/couchbase-manual-1.8/couchbase-monitoring-startup.html" target="_blank">monitor</a>
the warmup. This method was not only faster, but leaves 75% of our data intact and fast not to mention that we do not have to risk wasting time on <em>rebalance</em> failures.
At last but not least re-enable your <em>auto failover</em> when all of your nodes are done:</p>

<pre><code class="bash enable failover:">curl "http://localhost:8091/settings/autoFailover" -i -u Administrator:"yourpassword" -d 'enabled=true&amp;timeout=30'
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hands-on With Couchbase]]></title>
    <link href="http://variia.github.io/hands-on-with-couchbase/"/>
    <updated>2013-05-12T09:42:09+02:00</updated>
    <id>http://variia.github.io/hands-on-with-couchbase</id>
    <content type="html"><![CDATA[<p>It&rsquo;s been a long a long time coming, hard work has finally paid off and the last 7 months feels like just only few weeks.
<a href="http://www.couchbase.com" target="_blank">Couchbase</a> is now our primary NoSQL (key-value) store for production
and we are impressed with the results. This article is about our hands-on experience, benchmarking results and its associated
challenges.</p>

<!--more-->


<h2>Background</h2>

<p>We work in the online advertising market and for today&rsquo;s internet user speed is everything. Therefor, latency is paramount in
our application design thus, we needed something fast to store various user information including targeting data.
Few years ago, the choice was <a href="http://www.project-voldemort.com" target="_blank">Voldemort</a> for its latency and
speed, but unfortunately the product was not only vulnerable to cluster changes and disasters, but also was featuring small
user group so support was difficult. <code>memcached</code> always looked promising but the lack of clustering and disk persistence was
<em>too expensive</em> for our production suite.</p>

<p>Then Couchbase (membase with persistance backend) came along which was pretty new on the market, was going through couple of
re-brandings in a short period but it used <code>memcached</code> under the hood along with seamless clustering, auto-recovery and disk
persistence. Sounds like a dream? Well it was, but we had to wake up quickly in the middle of our migration because it was
just not going the way we wanted to.</p>

<h2>The Cluster</h2>

<p>As usual, one of our development team grabbed the Java SDK and started working on the code to implement a client in our production
suite. In the meantime we carved a new cluster out of 4 servers with the following specs:</p>

<ul>
<li>HP DL 360G7 Server</li>
<li>12 cores</li>
<li>256GB RAM</li>
<li>200GB SSD for disk persistence</li>
<li>CentOS 6.1</li>
<li>Couchbase Server 1.8.0</li>
</ul>


<p>It appeared to be a capable beast and after a short period, we started populating data to it in parallel with the existing Voldemort stores.
We loaded approximately 160 Million, keys to one of our high throughput buckets within a matter of weeks.</p>

<h2>The Failure</h2>

<p>Initial tests showed poor latencies of <code>5ms</code> and our application was throwing zillions of errors within seconds so we started refactoring.
We added <code>MBeans</code> to have visibility, gone over the SDK documentation number of times, added debugging metrics and it seemed whatever we do,
it is just not working out for us. We began monitoring the servers themselves and noticed that our utilization (context switching, interrupts)
do not seem to be affected by our application tests, so I started believing that we trip somewhere so early, that we don&rsquo;t even make it to the
&ldquo;buckets&rdquo;.</p>

<p>This went on for some time, even engaged the commercial support which was helpful to pinpoint potential issues in our setup, but we have not really
got much difference in results and our team really exhausted all of its options at that point of time. Couple of months later we felt that it&rsquo;s time
to look for something else and my team began looking at <a href="http://www.aerospike.com" target="_blank">AeroSpike</a> with the exception of me.
I simply refused to give up and was unable to accept failure.</p>

<h2>The Breakthrough</h2>

<p>Without much motivation I continued looking, and after so many hours of troubleshooting, Google searching I bumped into something promising we should
have (perhaps) looked at much earlier, the <a href="http://http://www.couchbase.com/wiki/display/couchbase/Java+Load+Generator" target="_blank">Java Load Test Generator</a>
from the community wiki. Disregarding our &ldquo;sprint&rdquo; plans, I grabbed a couple of developers along with the code and started a (somewhat) secret project
to work on a &ldquo;real load tester&rdquo; application. The code was a bit hard to read but we managed to modify it so it worked with our keys from the pre-populated
(160 million) data bucket not with some random generated garbage. We fired it from a single node and immediately managed to squeeze more juice out of the
cluster than ever before.</p>

<p>This is when the real development commenced with real, production data, on our production suite although we carried out the exercises off peak for our
customers&rsquo;s safety. We had to play with the setup to find the sweet spot and after a day of testing, we finally made a breakthrough. Of course it became
obvious that our <code>client</code> implementation was the problem not the product but without having another implementation alongside and of course enough
hands-on experience, it was impossible to prove. In a couple of weeks time, we completely refactored our client implementation based on the load tester
and I am happy to say that it&rsquo;s in production now for a good couple of months without a single glitch.</p>

<h2>Benchmark setup</h2>

<ul>
<li>Extracted 2 million keys out of our production bucket into a flat file, the load test client read this file into memory during startup</li>
<li>Ensured that all of our buckets are 100% memory resident (in-memory store only), even with SSDs reading is just too costly for us</li>
<li>Upgraded our cluster to version 1.8.1 and also patched it with <a href="http://support.couchbase.com/entries/21374979-TAP-disconnect-causes-memory-leak-in-1-8-x-MB-6550-" target="_blank">Hotfix MB-6550</a></li>
<li>Vacuumed all of our persisted databases on disk (SQLite) to ensure maximum performance (our production data population, [write only]
was constantly running during our benchmarks tests)</li>
<li>Copied the modified <code>ycsb.jar</code> along with the rest of the load test code to our NFS share</li>
<li>Adjusted the JVM to use 1G Max Heap along with the <a href="http://www.couchbase.com/docs/couchbase-sdk-java-1.0/java-gc-tuning.html" target="_blank">recommended JVM options</a> however the difference was neglectable</li>
<li>Executed the load test from 20 production application servers, all at once</li>
</ul>


<pre><code class="bash Test configuration:">db=com.yahoo.ycsb.db.MembaseClient
memcached.address=192.168.1.1
memcached.port=11211
histogram.buckets=20
exportfile=/tmp/results.txt
recordcount=100000
operationcount=2000000
workload=com.yahoo.ycsb.workloads.MemcachedCoreWorkload
readallfields=true
insertproportion=0
readproportion=0.100
updateproportion=0
scanproportion=0
memgetproportion=0.100
memupdateproportion=0.0
valuelength=2048
workingset=100000
churndelta=100000
printstatsinterval=5
requestdistribution=zipfian
threadcount=8
</code></pre>

<pre><code class="bash JVM tuning:">java -Xmx1g -XX:+UseConcMarkSweepGC -XX:MaxGCPauseMillis=850 -cp "lib/*:build/*" com.yahoo.ycsb.LoadGenerator -t -P loadtest.cfg
</code></pre>

<h2>Results and Conclusion</h2>

<p>  <img src="http://variia.github.io/images/2013-05/388BBE55-372F-4C56-86C8-61334B06579A.png" alt="console" /></p>

<ul>
<li>Couchbase scaled very well during our test, we achieved <strong>400 000 GET requests / second</strong> with an average latency of <strong>400us (micro-second)</strong>, 99th = 4ms
without using Multi-GET</li>
<li>CPU usage was not too heavy although we observed <strong>50K to 120K interrupts / second</strong> during load testing (per cluster node)</li>
<li>Increasing the client side <strong>threads beyond 16</strong> doubled the latency with <strong>only 20% increase</strong> in throughput, although it is likely to be caused by the
limitation of the client application (or its hardware)</li>
</ul>


<pre><code class="bash Results:">[OVERALL] RunTime(ms), 30008.0
[OVERALL] Throughput(ops/sec), 66648.89362836577
[GET] Operations, 2000000
[GET] AverageLatency, 401.87us
[GET] MinLatency, 146.0us
[GET] MaxLatency, 2.11s
[GET] 95thPercentileLatency, 2ms   
[GET] 99thPercentileLatency, 4ms   
[GET] 99.9thPercentileLatency, 16ms  
[GET] Return=0, 1989077
[GET] Return=-1, 10923
[GET] 0us    - 256us , 732757
[GET] 256us  - 512us , 1071121
[GET] 512us  - 1024us, 159357
[GET] 1024us - 2ms   , 22602
[GET] 2ms    - 4ms   , 2699
[GET] 4ms    - 8ms   , 10831
[GET] 8ms    - 16ms  , 403
[GET] 16ms   - 32ms  , 38
[GET] 32ms   - 64ms  , 0
[GET] 64ms   - 128ms , 160
[GET] 128ms  - 256ms , 0
[GET] 256ms  - 512ms , 0
[GET] 512ms  - 1024ms, 0
[GET] 1024ms - 2s    , 12
[GET] 2s     - 4s    , 20
[GET] 4s     - 8s    , 0
[GET] 8s     - 16s   , 0
[GET] 16s    - 32s   , 0
[GET] 32s    - 64s   , 0
[GET] 64s    - 128s  , 0
[GET] &amp;gt;128s  , 32&lt;/pre&gt;
</code></pre>

<h2>Final Verdict</h2>

<p>It seemed that Couchbase really shines between <strong>15% (~75K) and 80% (400K)</strong> of it&rsquo;s max throughput (520K). Low volume buckets (1-2 GETs / second or less)
only reach 600 - 700us regardless of disk write queue, resident ratio, etc. This suggests that perhaps there is some kind of <em>prefetch mechanism</em> for
actively requested data sets so if you request something frequently, you get better results. Adding more nodes with less memory would increase throughput,
improve bucket sharding, reduce network generated interrupts.</p>

<p>In nutshell: Couchbase performs better under pressure but you have to keep the pressure within 80% of your maximum throughput to maintain lowest latency.</p>
]]></content>
  </entry>
  
</feed>
